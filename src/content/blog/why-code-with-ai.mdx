---
title: "Why Code with AI?"
description: "A debate with myself on using AI as a developer — the fears, the pressures, and what actually matters."
date: "2025-01-01"
category: "tech"
tags: ["reflection", "tools"]
published: true
---

I've been wrestling with this question for a while now. Not because I don't use AI — I do — but because I'm not sure I've thought carefully enough about *why* I use it, and what it might cost me.

So here's my attempt to have that debate with myself, out loud.

## The Case Against

Let's start with the fear.

If I lean on AI too heavily, will I forget how to think? There's something to this. I've noticed it in small ways already — reaching for the assistant before I've even tried to solve the problem myself. The muscle that used to engage first now engages second, or not at all.

Coding isn't just about producing code. It's about building mental models, developing intuition, learning to sit with ambiguity until the shape of a solution emerges. If AI short-circuits that process, what's left of me as an engineer?

There's also the question of depth. When I copy-paste a solution I didn't write, I might get the output I wanted, but I don't always understand *why* it works. That gap compounds. Over time, I become a person who can ship, but not a person who can debug, extend, or explain.

And then there's craft. I got into programming because I liked the puzzle. The satisfaction of solving something myself. If I outsource the puzzle, what's the point?

## The Case For

Now the other side.

The world isn't waiting for me to figure this out. My peers are using AI. The industry expects more output, faster. Clients don't care if I wrote every line by hand — they care if the thing works and ships on time.

If I refuse to use AI on principle, I'm not noble. I'm just slow.

There's also a practical argument: AI handles the boring parts. Boilerplate, syntax I've written a thousand times, looking up APIs I'll forget tomorrow. If I can offload that, I free up mental space for the parts that actually matter — architecture, trade-offs, user experience.

And maybe the fear is overblown. I didn't forget how to do math when I started using a calculator. I didn't forget how to navigate when I got a smartphone. Tools change what we focus on, but they don't necessarily make us dumber.

## Where I've Landed (For Now)

I don't think AI is good or evil. It's a mirror. An amplifier.

If I'm lazy, AI makes me lazier. If I'm curious, it lets me explore faster. If I'm sloppy, it produces slop at scale. If I'm intentional, it clears the path so I can be *more* intentional.

The technology doesn't define me. How I use it does.

So I've started asking myself a few questions before reaching for the assistant:

1. **Do I understand the problem?** If not, AI won't help. It'll just give me a confident-sounding answer I can't evaluate.
2. **Am I learning or just shipping?** Both are valid, but I should know which mode I'm in.
3. **Could I do this without AI?** Not "would I" — just "could I." If the answer is no, that's a gap I need to fill, not ignore.

## The Deeper Question

This isn't really about AI. It's about how we relate to all technology.

Every tool we adopt shapes us. Cars changed how we think about distance. Smartphones changed how we think about attention. AI is changing how we think about knowledge and creativity.

The question isn't whether to use these tools. We will. The question is whether we use them *consciously* — aware of what we're gaining and what we might be losing.

I don't want to be the person who can't function without the tool. But I also don't want to be the person who refuses to adapt out of pride.

Somewhere in the middle is a posture that's honest about the trade-offs, intentional about the practice, and humble enough to keep asking the question.

---

I don't have this figured out. But I think the asking is the point.
